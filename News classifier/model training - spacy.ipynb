{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90cf86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all required libraries\n",
    "import spacy\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "import sys\n",
    "from spacy import displacy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0979d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT PROCESSING FUNCTION\n",
    "\n",
    "def clean_text(text ): \n",
    "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
    "    delete_dict[' '] = ' ' \n",
    "    table = str.maketrans(delete_dict)\n",
    "    text1 = text.translate(table)\n",
    "    #print('cleaned:'+text1)\n",
    "    textArr= text1.split()\n",
    "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>3))]) \n",
    "    \n",
    "    return text2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe8e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE DOCS TO FEED IT TO THE MODEL\n",
    "\n",
    "\n",
    "def make_docs(file_path):\n",
    "    \"\"\"\n",
    "    this will take a list of texts and labels \n",
    "    and transform them in spacy documents\n",
    "    \n",
    "    data: list(tuple(text, label))\n",
    "    \n",
    "    returns: List(spacy.Doc.doc)\n",
    "    \"\"\"\n",
    "    train_data = pd.read_csv(file_path)\n",
    "    train_data.dropna(axis = 0, how ='any',inplace=True) \n",
    "    train_data['Num_words_text'] = train_data['news'].apply(lambda x:len(str(x).split())) \n",
    "    mask = train_data['Num_words_text'] >2\n",
    "    train_data = train_data[mask]\n",
    "    print(train_data['label'].value_counts())\n",
    "    \n",
    "    \n",
    "    train_data['news'] = train_data['news'].apply(clean_text)\n",
    "   \n",
    "    data = tuple(zip(train_data['news'].tolist(), train_data['label'].tolist())) \n",
    "    print(data[1])\n",
    "    docs = []\n",
    "    # nlp.pipe([texts]) is way faster than running \n",
    "    # nlp(text) for each text\n",
    "    # as_tuples allows us to pass in a tuple, \n",
    "    # the first one is treated as text\n",
    "    # the second one will get returned as it is.\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)):\n",
    "        \n",
    "        # we need to set the (text)cat(egory) for each document\n",
    "        #print(label)\n",
    "        if (label=='sports'):\n",
    "            doc.cats['sports'] = 1\n",
    "            doc.cats['tech'] = 0\n",
    "            \n",
    "            doc.cats['general'] = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        elif (label=='general'):\n",
    "            doc.cats['sports'] = 0\n",
    "            doc.cats['tech'] = 0\n",
    "            \n",
    "            doc.cats['general'] = 1\n",
    "        \n",
    "        else:\n",
    "            doc.cats['sports'] = 0\n",
    "            doc.cats['tech'] = 1\n",
    "            \n",
    "            doc.cats['general'] = 0\n",
    "        #print(doc.cats)\n",
    "        \n",
    "        # put them into a nice list\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs,train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f1d1f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tech       757\n",
      "general    689\n",
      "sports     669\n",
      "Name: label, dtype: int64\n",
      "('infrastructures able interrupt botnet activity warns that temporarily once glupteba uses blockchain technology mechanism avoid comp', 'tech')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdf8a2176e24c44817a2429edb136aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidfe\\anaconda3\\envs\\ironhack\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tech       193\n",
      "sports     176\n",
      "general    160\n",
      "Name: label, dtype: int64\n",
      "('real american leaders start from today until next vodafone invites your customers choose christmas offer that want have access just through vodafone selected', 'tech')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b98a6487e19457f8dbdc2dc717a247f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/529 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##LOAD TRAIN/TEST DATA\n",
    "\n",
    "train_docs,train_data  = make_docs(\"train.csv\") #path to train data\n",
    "# then we save it in a binary file to disc\n",
    "doc_bin = DocBin(docs=train_docs)\n",
    "doc_bin.to_disk(\"textcat_train.spacy\")\n",
    "\n",
    "test_docs,test_data  = make_docs(\"test.csv\") #path to test data\n",
    "# then we save it in a binary file to disc\n",
    "doc_bin = DocBin(docs=test_docs)\n",
    "doc_bin.to_disk(\"textcat_valid.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b50b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "textcat_config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train textcat_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "##WE NEED TO HAVE THE TEXTCAT_BASE_CONFIG FILE  (EITHER STICK WITH MINE OR GET A NEW ONE FROM SPACY WEBSITE)\n",
    "\n",
    "\n",
    "!python -m spacy init fill-config ./textcat_base_config.cfg ./textcat_config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87383d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Created output directory: textcat_output\n",
      "[i] Saving to output directory: textcat_output\n",
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['transformer', 'textcat']\n",
      "[i] Initial learn rate: 0.0\n",
      "E    #       LOSS TRANS...  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  -------------  ------------  ----------  ------\n",
      "  0       0           0.00          0.01        0.00    0.00\n",
      "  8     200           0.01         21.72       84.82    0.85\n",
      " 17     400           0.07          1.37       87.12    0.87\n",
      " 26     600           0.00          0.02       85.90    0.86\n",
      " 35     800           0.00          0.00       86.15    0.86\n",
      " 43    1000           0.00          0.00       85.18    0.85\n",
      " 52    1200           0.00          0.00       87.35    0.87\n",
      " 61    1400           0.00          0.00       87.21    0.87\n",
      " 69    1600           0.00          0.00       86.07    0.86\n",
      " 78    1800           0.00          0.00       86.49    0.86\n",
      " 87    2000           0.00          0.00       87.46    0.87\n",
      " 96    2200           0.00          0.00       86.34    0.86\n",
      "104    2400           0.01          0.03       85.17    0.85\n",
      "113    2600           0.00          0.00       86.12    0.86\n",
      "122    2800           0.00          0.00       87.47    0.87\n",
      "131    3000           0.00          0.03       88.06    0.88\n",
      "140    3200           0.00          0.00       86.37    0.86\n",
      "149    3400           0.00          0.00       87.59    0.88\n",
      "158    3600           0.00          0.00       88.24    0.88\n",
      "166    3800           0.00          0.00       87.19    0.87\n",
      "175    4000           0.00          0.00       87.98    0.88\n",
      "184    4200           0.00          0.00       86.63    0.87\n",
      "193    4400           0.00          0.00       87.26    0.87\n",
      "201    4600           0.14          1.85       87.90    0.88\n",
      "210    4800           0.08          0.25       87.56    0.88\n",
      "219    5000           0.00          0.00       86.52    0.87\n",
      "228    5200           0.00          0.00       85.43    0.85\n",
      "[+] Saved pipeline to output directory\n",
      "textcat_output\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-13 21:43:13,714] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "[2021-12-13 21:43:14,222] [INFO] Set up nlp object from config\n",
      "[2021-12-13 21:43:14,230] [DEBUG] Loading corpus from path: textcat_valid.spacy\n",
      "[2021-12-13 21:43:14,231] [DEBUG] Loading corpus from path: textcat_train.spacy\n",
      "[2021-12-13 21:43:14,231] [INFO] Pipeline: ['transformer', 'textcat']\n",
      "[2021-12-13 21:43:14,235] [INFO] Created vocabulary\n",
      "[2021-12-13 21:43:14,235] [INFO] Finished initializing nlp object\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\sidfe\\anaconda3\\envs\\ironhack\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "[2021-12-13 21:43:25,125] [INFO] Initialized pipeline components: ['transformer', 'textcat']\n",
      "[2021-12-13 21:43:25,134] [DEBUG] Loading corpus from path: textcat_valid.spacy\n",
      "[2021-12-13 21:43:25,134] [DEBUG] Loading corpus from path: textcat_train.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train textcat_config.cfg --verbose --output ./textcat_output --paths.train textcat_train.spacy --paths.dev textcat_valid.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d21cc0c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_23312/1525340615.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnlp_textcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"textcat_output/model-best\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'news'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_cats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdoc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp_textcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Text: \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mtest_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "#LOAD AND TEST THE MODEL\n",
    "\n",
    "\n",
    "nlp_textcat = spacy.load(\"textcat_output/model-best\")\n",
    "test_texts = test_data['news'].tolist()\n",
    "test_cats = test_data['label'].tolist()\n",
    "doc2 = nlp_textcat(test_texts[1])\n",
    "print(\"Text: \"+ test_texts[1])\n",
    "print(\"Orig Cat:\"+ test_cats[1])\n",
    "print(\" Predicted Cats:\") \n",
    "print(doc2.cats)\n",
    "print(\"=======================================\")\n",
    "doc2 = nlp_textcat(test_texts[200])\n",
    "print(\"Text: \"+ test_texts[200])\n",
    "print(\" Orig Cat:\"+test_cats[200])\n",
    "print(\" Predicted Cats:\") \n",
    "print(doc2.cats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
